# Mini-Project-2-Introduction-to-LLMs-and-Gen-AI
# ðŸ“Š Sentiment Analysis with NLP & Word Embeddings  

## ðŸ“Œ Project Overview  
This project is part of **Mini Project 2: Basics of NLP â€“ Text Cleaning & Vectorization** under *Introduction to LLMs and Generative AI*.  

The goal is to build an **AI-powered sentiment analysis pipeline** that processes customer reviews from an e-commerce platform and classifies them into **Positive, Negative, or Neutral** sentiments.  
It demonstrates **text preprocessing, vectorization (BoW, Word2Vec, GloVe)**, and prepares data for downstream machine learning models.  

---

## ðŸš€ Problem Statement  

A growing e-commerce platform in **electronic gadgets** faces:  
- **200% growth** in customer base (last 3 years)  
- **25% spike** in feedback volume  

Manual review is no longer feasible.  
Business challenges include:  
- Customer Churn  
- Reputation Damage  
- Financial Loss  

The company needs an **AI-driven sentiment classification system**.  

---

## ðŸ“‚ Dataset  
- **Product ID** â†’ Unique identifier for each product  
- **Product Review** â†’ Customer feedback text  
- **Sentiment** â†’ Positive / Negative / Neutral  
- **Size** â†’ 1007 rows Ã— 3 columns  

---

## ðŸ› ï¸ Methodology  

### ðŸ”¹ 1. Data Preprocessing  
- Removed special characters  
- Converted text to lowercase  
- Stripped extra whitespaces  
- Removed stopwords (NLTK)  
- Applied stemming (**Porter Stemmer**)  
- Applied lemmatization (**WordNet Lemmatizer**)  

### ðŸ”¹ 2. Exploratory Data Analysis (EDA)  
- Sentiment distribution (class imbalance observed)  
- Word frequency visualization  

### ðŸ”¹ 3. Feature Engineering & Vectorization  
- **Bag of Words (BoW)**  
- **Word2Vec** (CBOW & Skip-gram)  
- **GloVe embeddings**  

### ðŸ”¹ 4. Evaluation Metrics  
- **Macro F1-Score** â†’ balances all classes equally  
- **Per-class Precision & Recall**  
- **Confusion Matrix**  

---

## ðŸ“ˆ Key Insights  
- Dataset is **imbalanced** (~85% Positive reviews).  
- **Macro F1** better reflects performance than accuracy.  
- **Word embeddings (Word2Vec & GloVe)** capture semantic meaning better than BoW.  

---

## ðŸ—ï¸ Tech Stack  
- **Python** (Pandas, NumPy, Scikit-learn)  
- **NLTK / Gensim** â†’ text processing & embeddings  
- **Matplotlib / Seaborn** â†’ visualizations  
- **Google Colab** â†’ implementation environment  

---

## ðŸ“Š Workflow  
```mermaid
flowchart TD
    A[Raw Data] --> B[Preprocessing]
    B --> C[EDA & Visualization]
    C --> D[Vectorization]
    D --> E[Word2Vec & GloVe Embeddings]
    E --> F[Model Training & Evaluation]
    F --> G["Deployment - Future Scope"]

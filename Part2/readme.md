# Mini Project 2 â€“ Part 2  
### Automated News Article Categorization with NLP & ML  

---

## ğŸ“Œ Business Context  
In todayâ€™s fast-moving media landscape, rapid categorization and curation are essential. With articles pouring in across diverse topics, publishers need efficient systems to get the right stories to the right audience on timeâ€”crucial for retention amid information overload.  

**Challenges:**  
- **Information Overload** â€“ Manual tagging is impractical at scale.  
- **Timeliness** â€“ Delays lead to outdated or misplaced content.  
- **Engagement** â€“ Personalization & relevance are vital for user retention.  

**Case Study:**  
InfoWorld, a major publisher, handles a vast repository of articles across global affairs, entertainment, politics, business, and more. Manual categorization often causes delays and inaccuracies. To optimize this, InfoWorld aims to adopt **machine learning** for automated and accurate categorization.  

---

## ğŸ¯ Problem Definition  
As a **data scientist** on InfoWorldâ€™s team, the goal is to build a **predictive model** that categorizes articles automatically. This ensures:  
- Faster processing of incoming content  
- Personalized content delivery  
- Improved accuracy in tagging  

---

## ğŸ“‚ Dataset Information  
- **File:** `Articles.csv`  
- **Shape:** 4076 articles Ã— 7 features  
- **Columns:**  
  - `Date published`  
  - `Category`  
  - `Section`  
  - `Headline`  
  - `Description`  
  - `Keywords`  
  - `Article text`  

---

## ğŸ” Exploratory Data Analysis (EDA)  
- No missing values found.  
- No duplicate rows.  
- **Category Distribution:**  
  - Sport (53.4%)  
  - News (39.5%)  
  - Others: Business, Politics, Entertainment, Health (remaining)  

- **Section Distribution:** Highly imbalanced across ~30+ sections, with Sport, Europe, and Football dominating.  
- Extracted **publication year** from dates for trend analysis.  

---

## ğŸ› ï¸ Data Preprocessing  
1. Removal of special characters (`regex`).  
2. Lowercasing all text.  
3. Removing extra whitespace.  
4. Stopword removal (NLTK).  
5. Lemmatization (WordNet Lemmatizer).  
6. Final cleaned text stored in `final_cleaned_text`.  

---

## ğŸ”¡ Text Vectorization  
- **Count Vectorizer (BoW)** â€“ Frequency-based representation.  
- **Word Embeddings (GloVe 100D)** â€“ Semantic vectorization using pre-trained embeddings.  
- Custom **average vectorizer** implemented to represent documents.  

---

## ğŸ¤– Modeling  
- **Algorithms considered:**  
  - Random Forest Classifier (baseline).  
  - GridSearchCV for hyperparameter tuning.  
- **Evaluation Metrics:**  
  - Accuracy Score  
  - Classification Report (Precision, Recall, F1)  
  - Confusion Matrix  

---

## ğŸ“¦ Libraries Used  
- `pandas`, `numpy`, `matplotlib`, `seaborn` â€“ Data handling & visualization  
- `nltk` â€“ Stopwords, Lemmatizer  
- `re` â€“ Regex-based cleaning  
- `sklearn` â€“ ML models, metrics, preprocessing  
- `gensim` â€“ Word2Vec & GloVe embeddings  

---

## ğŸš€ Next Steps  
- Experiment with **deep learning models** (LSTM, BERT).  
- Improve handling of **imbalanced categories**.  
- Deploy as a **real-time categorization service**.  

---

## ğŸ‘©â€ğŸ’» Author  
**Adnan Parwez**  
B.Tech. Computer Science  

---

 

